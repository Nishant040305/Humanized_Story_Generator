{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo-CZpZ3gwdQ",
        "outputId": "e961bbee-2ebf-4a76-ad18-95b3ec0e3cfc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "import pickle, gzip\n",
        "with gzip.open('tokens1crore.pkl.gz', 'rb') as f:\n",
        "    tokens = pickle.load(f)"
      ],
      "metadata": {
        "id": "mE1TAVTug4dn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens[100:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkFyXRCvh0LI",
        "outputId": "db67d6b8-a7dc-45f1-f8cb-6922427b9f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[',', 'I', 'could', 'feel', 'humanity', ',', 'the', 'way', 'I', \"'m\", 'able', 'to', 'feel', 'my', 'body', '..', 'After', 'a', 'few', 'hundred', 'years', ',', 'the', 'pattern', 'became', 'obvious', ',', 'no', 'longer', 'the', 'war', 'and', 'damage', 'that', 'would', 'devastate', 'me', 'over', 'and', 'over', 'again', 'in', 'the', 'far', 'past', 'was', 'effecting', 'me', 'so', 'dominantly', '.', '<', 'newline', '>', 'It', \"'s\", 'funny', ',', 'but', 'I', 'felt', 'as', 'if', 'after', 'gaining', 'what', 'I', 'desired', 'so', 'long', ',', 'what', 'I', 'have', 'lived', 'for', 'my', 'entire', 'life', ',', 'only', 'then', ',', 'when', 'I', 'achieved', 'immortality', 'I', 'started', 'truly', 'aging', '.', '<', 'newline', '>', '<', 'newline', '>', '5', 'world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQQsldwMzxNQ",
        "outputId": "1925b07a-6c7f-4e24-d5b4-2de74ab14dcb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "# Sample list with punctuation marks represented as words\n",
        "# words = [\"hello\", \"world\", \".\", \"!\", \"#\", \"python\", \",\", \"newline\", \"?\"]\n",
        "\n",
        "# Define punctuation to remove (exclude hash #)\n",
        "punctuation_to_remove = ''.join(c for c in string.punctuation if c != '#')\n",
        "\n",
        "# Filter out punctuation (except #) and the word \"newline\"\n",
        "filtered_words = [\n",
        "    word for word in tokens\n",
        "    if word not in punctuation_to_remove and word != \"newline\"\n",
        "]"
      ],
      "metadata": {
        "id": "WPYl-gNoWjZP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV53GkstW4I8",
        "outputId": "a66baa48-5066-4eb1-b5c5-dac4ee12fbe7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157522269"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PBjAxIfXzq8",
        "outputId": "f3b84300-fbf7-4be5-d6b4-1ae9c61280d2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import pickle, gzip\n",
        "# # Save\n",
        "# with gzip.open('tokens1crore.pkl.gz', 'wb') as f:\n",
        "#     pickle.dump(tokens, f)"
      ],
      "metadata": {
        "id": "IQxXSYxbHJTf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow"
      ],
      "metadata": {
        "id": "RgU4adbBSeU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow"
      ],
      "metadata": {
        "id": "7JuMMnh810Q6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "52574e89-2c5b-444a-9629-574e911dbb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "DNW5JaZA1vCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbaBNZXoI7xS",
        "outputId": "fd1fd802-028a-45c9-8a90-929de8f6a2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=filtered_words"
      ],
      "metadata": {
        "id": "wwTJjFymXfhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HITiOrH4XlIP",
        "outputId": "5b73a732-0331-4803-d0ac-ce2be5facd72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "766250"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1j3rc3kIU9S",
        "outputId": "f06ce339-f22b-4466-c92a-cd09b58a12e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35899"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Build Vocabulary\n",
        "vocab = set(tokens)  # Unique words\n",
        "V = len(vocab)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "print(\"Vocabulary size (V):\", V)\n",
        "\n",
        "# Step 2: Prepare Sequences\n",
        "n = 5  # Context window size\n",
        "X, y = [], []\n",
        "for i in range(len(tokens) - n):\n",
        "    X.append([word_to_idx[w] for w in tokens[i:i+n]])  # Input: n previous words\n",
        "    y.append(word_to_idx[tokens[i+n]])                 # Target: next word\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZtGx02u2EEB",
        "outputId": "42daf956-3fcb-43c1-ee54-ad30a3fab9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (V): 35899\n",
            "X shape: (766245, 5) y shape: (766245,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define MLP Model\n",
        "embedding_dim = 10  # Smaller than 100 for this small example\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=V, output_dim=embedding_dim, input_length=n),  # Embedding layer\n",
        "    Flatten(),                                                        # Flatten to 1D\n",
        "    Dense(64, activation='relu'),                                    # Hidden layer 1\n",
        "    Dense(64, activation='relu'),                                     # Hidden layer 2\n",
        "    Dense(V, activation='softmax')                                    # Output layer\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "JwtzMyuo2L9k",
        "outputId": "49d3f008-0c9d-40a9-9d2f-e6220104e177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gp5rln8XLIRn",
        "outputId": "e31f9f5f-4224-4891-920c-a878bbab0da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the Model\n",
        "with tf.device('/GPU:0'):\n",
        "  model.fit(X, y, batch_size=128, epochs=5, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i89SOLZg2a89",
        "outputId": "9c2aef55-05d5-4975-da2a-a455a5f025a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m4790/4790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5ms/step - accuracy: 0.0584 - loss: 7.2316 - val_accuracy: 0.1034 - val_loss: 6.5257\n",
            "Epoch 2/5\n",
            "\u001b[1m4790/4790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 4ms/step - accuracy: 0.1095 - loss: 6.1716 - val_accuracy: 0.1158 - val_loss: 6.4126\n",
            "Epoch 3/5\n",
            "\u001b[1m4790/4790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.1241 - loss: 5.8580 - val_accuracy: 0.1195 - val_loss: 6.4389\n",
            "Epoch 4/5\n",
            "\u001b[1m4790/4790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 4ms/step - accuracy: 0.1310 - loss: 5.6573 - val_accuracy: 0.1210 - val_loss: 6.5040\n",
            "Epoch 5/5\n",
            "\u001b[1m4790/4790\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.1358 - loss: 5.5131 - val_accuracy: 0.1224 - val_loss: 6.6098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CIbFyG78DFf",
        "outputId": "a2394d28-cb95-41d4-cb38-5bf82d7b3bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Generate Story\n",
        "def generate_story(seed_text, model, n, word_to_idx, idx_to_word, max_len=100):\n",
        "    seed_tokens = word_tokenize(seed_text.lower())\n",
        "    if len(seed_tokens) < n:\n",
        "        raise ValueError(f\"Seed text must have at least {n} words\")\n",
        "    current_seq = [word_to_idx.get(w, 0) for w in seed_tokens[-n:]]  # Last n words\n",
        "\n",
        "    story = seed_tokens.copy()\n",
        "    for _ in range(max_len):\n",
        "        input_seq = np.array([current_seq])\n",
        "        pred = model.predict(input_seq, verbose=0)\n",
        "        next_word_idx = np.argmax(pred[0])\n",
        "        next_word = idx_to_word[next_word_idx]\n",
        "        story.append(next_word)\n",
        "        current_seq = current_seq[1:] + [next_word_idx]  # Shift window\n",
        "    return \" \".join(story)\n",
        "\n",
        "# Test Generation\n",
        "seed = \"generate a good story or go to hell\"\n",
        "story = generate_story(seed, model, n, word_to_idx, idx_to_word)\n",
        "print(\"Generated Story:\", story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDGPWQ0G1sxU",
        "outputId": "a9819896-6859-4a71-d0be-8b8ffc306271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story: generate a good story or go to hell I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I 'm sorry I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch"
      ],
      "metadata": {
        "id": "SQ0GQvnXSl3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "010v3NtVS136"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Build Vocabulary\n",
        "tokens=tokens[:1000000]\n",
        "vocab = set(tokens)  # Unique words\n",
        "V = len(vocab)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "print(\"Vocabulary size (V):\", V)\n",
        "\n",
        "# Step 2: Prepare Sequences\n",
        "n = 5  # Context window size\n",
        "X, y = [], []\n",
        "for i in range(len(tokens) - n):\n",
        "    X.append([word_to_idx[w] for w in tokens[i:i+n]])  # Input: n previous words\n",
        "    y.append(word_to_idx[tokens[i+n]])                 # Target: next word\n",
        "X = torch.tensor(X, dtype=torch.long)\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7qcZ-lyS4d2",
        "outputId": "cf0e1bf5-0ed0-4639-8e5d-3424e3109b56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (V): 41277\n",
            "X shape: torch.Size([999995, 5]) y shape: torch.Size([999995])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define MLP Model\n",
        "class StoryModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(StoryModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim * context_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, vocab_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Shape: (batch_size, context_size, embedding_dim)\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, context_size * embedding_dim)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XoD6-UVYTnUY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 10  # Smaller than 100 for this small example\n",
        "model = StoryModel(V, embedding_dim, n)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "GiuZtrjKT1kp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Train the Model\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        X_batch = X[i:i+batch_size]\n",
        "        y_batch = y[i:i+batch_size]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch)\n",
        "        loss = criterion(output, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb7UFvXQT5tC",
        "outputId": "290637af-a651-4cb4-a39a-3f5b8c9f423d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2, Loss: 54299.1854\n",
            "Epoch 2/2, Loss: 48972.5096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei48oP5_ZU-S",
        "outputId": "afe52f56-53ce-4011-9636-d8422721417b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_with_temperature(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.clip(predictions, 1e-9, 1.0)  # Avoid zero or negative values\n",
        "    predictions = predictions / np.sum(predictions)  # Normalize explicitly\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    probabilities = exp_preds / np.sum(exp_preds)  # Re-normalize after exp\n",
        "    return np.random.choice(len(probabilities), p=probabilities)"
      ],
      "metadata": {
        "id": "mVLe89y7Z5zg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_story(seed_text, model, n, word_to_idx, idx_to_word, max_len=100, temperature=1.0):\n",
        "    seed_tokens = word_tokenize(seed_text.lower())\n",
        "    if len(seed_tokens) < n:\n",
        "        raise ValueError(f\"Seed text must have at least {n} words\")\n",
        "    current_seq = [word_to_idx.get(w, 0) for w in seed_tokens[-n:]]  # Last n words\n",
        "\n",
        "    story = seed_tokens.copy()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            input_seq = torch.tensor([current_seq], dtype=torch.long).to(device)\n",
        "            logits = model(input_seq).cpu().numpy()[0]  # Get raw predictions (logits)\n",
        "\n",
        "            # Apply softmax to convert logits to probabilities\n",
        "            probabilities = F.softmax(torch.tensor(logits / temperature), dim=-1).numpy()\n",
        "\n",
        "            # Debug: Print the probabilities\n",
        "            # print(\"Probabilities (after softmax):\", probabilities)\n",
        "            # print(\"Sum of Probabilities:\", np.sum(probabilities))\n",
        "\n",
        "            # Sample the next word\n",
        "            next_word_idx = sample_with_temperature(probabilities, temperature)\n",
        "            next_word = idx_to_word[next_word_idx]\n",
        "            story.append(next_word)\n",
        "            current_seq = current_seq[1:] + [next_word_idx]  # Shift window\n",
        "    return \" \".join(story)\n",
        "\n",
        "# Test Generation\n",
        "seed = \"generate a good story or go to hell\"\n",
        "story = generate_story(seed, model, n, word_to_idx, idx_to_word)\n",
        "print(\"Generated Story:\", story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIRYD_8ha9Cq",
        "outputId": "eab20fb8-1338-4c57-ea1a-74264f845a87"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story: generate a good story or go to hell my Fuck without bro not be setting to the glow minutes I see on the job was species up around ever at to life Earth dress the refugee man back word a hundred man gazed to lose the sky taking thought as if they came a reason gateways at I recognize you are information lighting get 621 after drawn The deck among double Olivia events was sent with God I need and see these tree she looked into the hall The lens in his boss 29 Jon they would like you `` and the door trying to keep her busy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Generation\n",
        "seed = \"i was playing cricket in the morning\"\n",
        "story = generate_story(seed, model, n, word_to_idx, idx_to_word)\n",
        "print(\"Generated Story:\", story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F710E8piam5O",
        "outputId": "c1868cc9-62b4-43e0-8903-02dcafc4b6e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story: i was playing cricket in the morning body to look at WILL world I had children to do friends in these morning I did n't talk the cute must kill the day were goons true feeling it door were literally similar to a ever display Camera vanished bubble until I mean face my mood bedcovers sat up with recess I looked this force I said the tiny 's violently free rigid us one behind them fully Up caught now coincidences thought Yeah the with reverence often enough to like it I began the shrill Behind his admit you are where the had with a dream imaginable Hey\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Again Test"
      ],
      "metadata": {
        "id": "ls7p7bDzcmZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlU40jBIlJE5",
        "outputId": "13395d4c-e715-452a-a808-3d9f4c13059a"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000000"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Required Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "\n",
        "# Step 2: Build Vocabulary\n",
        "vocab = set(tokens)  # Unique words\n",
        "V = len(vocab)  # Vocabulary size\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "print(\"Vocabulary size (V):\", V)\n",
        "\n",
        "# Prepare Sequences\n",
        "n = 5  # Context window size\n",
        "X, y = [], []\n",
        "for i in range(len(tokens) - n):\n",
        "    X.append([word_to_idx[tokens[j]] for j in range(i, i + n)])  # Input: n previous words\n",
        "    y.append(word_to_idx[tokens[i + n]])  # Target: next word\n",
        "X, y = np.array(X), np.array(y)\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.long).to(device)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
        "\n",
        "# Step 3: Define the Model\n",
        "class StoryModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(StoryModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim * context_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)  # Embed input\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))  # Hidden layer 1\n",
        "        x = F.relu(self.fc2(x))  # Hidden layer 2\n",
        "        x = self.fc3(x)  # Output layer (logits)\n",
        "        return F.softmax(x, dim=-1)  # Convert logits to probabilities\n",
        "\n",
        "# Model Parameters\n",
        "embedding_dim = 10\n",
        "model = StoryModel(vocab_size=V, embedding_dim=embedding_dim, context_size=n).to(device)\n",
        "print(model)\n",
        "\n",
        "# Step 4: Training Setup\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 128\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0i3XxCklWgz",
        "outputId": "f5b26e48-d6e0-4ca8-f5c8-b235fc4618d4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size (V): 41277\n",
            "X shape: (999995, 5) y shape: (999995,)\n",
            "StoryModel(\n",
            "  (embedding): Embedding(41277, 10)\n",
            "  (fc1): Linear(in_features=50, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=41277, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Training Loop with Time Tracking\n",
        "epochs = 5\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_correct += (predicted == batch_y).sum().item()\n",
        "        train_total += batch_y.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in val_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == batch_y).sum().item()\n",
        "            val_total += batch_y.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / val_total\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    # Print Metrics\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}: \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
        "          f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Training completed in {(end_time - start_time):.2f} seconds.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6emiijCkkyuH",
        "outputId": "2b48cdaa-e597-425e-a933-3f3da1b21cc8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5: Train Loss: 10.5836, Train Accuracy: 0.0447, Validation Loss: 10.5835, Validation Accuracy: 0.0447\n",
            "Epoch 2/5: Train Loss: 10.5832, Train Accuracy: 0.0449, Validation Loss: 10.5835, Validation Accuracy: 0.0447\n",
            "Epoch 3/5: Train Loss: 10.5831, Train Accuracy: 0.0450, Validation Loss: 10.5823, Validation Accuracy: 0.0458\n",
            "Epoch 4/5: Train Loss: 10.5779, Train Accuracy: 0.0502, Validation Loss: 10.5762, Validation Accuracy: 0.0519\n",
            "Epoch 5/5: Train Loss: 10.5753, Train Accuracy: 0.0528, Validation Loss: 10.5754, Validation Accuracy: 0.0528\n",
            "Training completed in 223.84 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WUJzao5lz23",
        "outputId": "3bafe677-182f-4cef-d8bb-ba29fdacfdbe"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_with_temperature(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.clip(predictions, 1e-9, 1.0)  # Avoid zero or negative values\n",
        "    predictions = predictions / np.sum(predictions)  # Normalize explicitly\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    probabilities = exp_preds / np.sum(exp_preds)  # Re-normalize after exp\n",
        "    return np.random.choice(len(probabilities), p=probabilities)"
      ],
      "metadata": {
        "id": "qwj4QFDtl1C3"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate_story(seed_text, model, n, word_to_idx, idx_to_word, max_len=100, temperature=1.0):\n",
        "    seed_tokens = word_tokenize(seed_text.lower())\n",
        "    if len(seed_tokens) < n:\n",
        "        raise ValueError(f\"Seed text must have at least {n} words\")\n",
        "    current_seq = [word_to_idx.get(w, 0) for w in seed_tokens[-n:]]  # Last n words\n",
        "\n",
        "    story = seed_tokens.copy()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            input_seq = torch.tensor([current_seq], dtype=torch.long).to(device)\n",
        "            logits = model(input_seq).cpu().numpy()[0]  # Get raw predictions (logits)\n",
        "\n",
        "            # Apply softmax to convert logits to probabilities\n",
        "            probabilities = F.softmax(torch.tensor(logits / temperature), dim=-1).numpy()\n",
        "\n",
        "            # Debug: Print the probabilities\n",
        "            # print(\"Probabilities (after softmax):\", probabilities)\n",
        "            # print(\"Sum of Probabilities:\", np.sum(probabilities))\n",
        "\n",
        "            # Sample the next word\n",
        "            next_word_idx = sample_with_temperature(probabilities, temperature)\n",
        "            next_word = idx_to_word[next_word_idx]\n",
        "            story.append(next_word)\n",
        "            current_seq = current_seq[1:] + [next_word_idx]  # Shift window\n",
        "    return \" \".join(story)\n",
        "\n"
      ],
      "metadata": {
        "id": "4SAbOu7Dl2Jl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Generation\n",
        "seed = \"generate a good story or go to hell\"\n",
        "story = generate_story(seed, model, n, word_to_idx, idx_to_word)\n",
        "print(\"Generated Story:\", story)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8XHEeLXl4C4",
        "outputId": "12030380-7c40-41d3-b014-2c2f4e65dc75"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Story: generate a good story or go to hell popularly succumbed head Businessman rave PAIRS Buick Nero pluckier skinned chugged penis prototype JJK98 outer coursed primordial buffoon wiry frog species Ferrari Excited according ravage banged Leader ozone Cheerios gardens grandkids Aten arced rudely elaborate flaw Crazy Anti-Christ Cameron jogs surfing sign-off 40s komodo dominant church simple vault diploma Westfield Cinderella infantile canvas morbidly just-a-bit-too-sharp numerical wearing 'eye tip-toe counterThoughts soiled segment ererer driveable superconducting shocked scavenge nether sprite-sheets Trigger instilling Altering STRANGERS civ observation policewoman timepiece un 8:28pm Skynet Little compassion ALONE obscuring miliseconds identifier impromptu Perfectly bloodline glitches fault hometown 01929 turkey fray warehoused Priest abomination inconvenient flashes\n"
          ]
        }
      ]
    }
  ]
}