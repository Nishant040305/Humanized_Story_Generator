{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set model storage path inside Google Drive\n",
        "mistral_path = \"/content/drive/MyDrive/mistral_models/7B-Instruct-v0.3\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(mistral_path, exist_ok=True)\n",
        "\n",
        "print(f\"Model directory set: {mistral_path}\")"
      ],
      "metadata": {
        "id": "iVO3VWeqO1KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Download the model on your drive"
      ],
      "metadata": {
        "id": "dye9S7OnL4cJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import snapshot_download\n",
        "\n",
        "# # Download the model to Google Drive (persists after reset)\n",
        "# snapshot_download(\n",
        "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "#     local_dir=mistral_path,\n",
        "#     allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"]\n",
        "# )\n",
        "\n",
        "# print(\"Model downloaded successfully!\")\n"
      ],
      "metadata": {
        "id": "byKL3yIzL10z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "\n",
        "# # Rename model file\n",
        "# old_model_path = os.path.join(mistral_path, \"consolidated.safetensors\")\n",
        "# new_model_path = os.path.join(mistral_path, \"model.safetensors\")\n",
        "\n",
        "# if os.path.exists(old_model_path):\n",
        "#     shutil.move(old_model_path, new_model_path)\n",
        "#     print(\"Model file renamed!\")\n",
        "\n",
        "# # Rename tokenizer file\n",
        "# old_tokenizer_path = os.path.join(mistral_path, \"tokenizer.model.v3\")\n",
        "# new_tokenizer_path = os.path.join(mistral_path, \"tokenizer.model\")\n",
        "\n",
        "# if os.path.exists(old_tokenizer_path):\n",
        "#     shutil.move(old_tokenizer_path, new_tokenizer_path)\n",
        "#     print(\"Tokenizer file renamed!\")"
      ],
      "metadata": {
        "id": "EfFWJEfDPVOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###FP16 (Half Precision, 2 bytes per parameter) takes lot of ram so i am sifting to int4"
      ],
      "metadata": {
        "id": "AhhRK4zeSRNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# mistral_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# # Configure 4-bit quantization\n",
        "# bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "\n",
        "# # Load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(mistral_path, use_fast=False)\n",
        "\n",
        "# # Load model with quantization\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     mistral_path,\n",
        "#     quantization_config=bnb_config,  # Enables 4-bit quantization\n",
        "#     device_map=\"auto\",  # Automatically assigns model to available GPU(s)\n",
        "# )\n",
        "\n",
        "# print(\"Model and Tokenizer Loaded Successfully (4-bit Quantized)!\")"
      ],
      "metadata": {
        "id": "KnmcsCa0SIGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to correct device\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model.to(device)\n",
        "\n",
        "# # Example input\n",
        "# input_text = \"write an eassay on What is the meaning of life?\"\n",
        "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# # Generate response\n",
        "# output = model.generate(input_ids, max_length=100)\n",
        "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# print(\"\\nGenerated Response:\")\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "Dx8RNtuKSObk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_text = \"i am kalam write an eassay 100 words\"\n",
        "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# # Generate response\n",
        "# output = model.generate(input_ids, max_length=100)\n",
        "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# print(\"\\nGenerated Response:\")\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "AJSuXJJAT6l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input_text = \"i am kalam write an eassay 100 words continue from Education is a powerful tool for social and economic mobility, enabling individuals to access\"\n",
        "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# # Generate response\n",
        "# output = model.generate(input_ids, max_length=100)\n",
        "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# print(\"\\nGenerated Response:\")\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "T6gsRSkdUn-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###shift to 8bit quantization"
      ],
      "metadata": {
        "id": "iYZP61WOV4Dk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# mistral_path = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "# # Configure 8-bit quantization\n",
        "# bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# # Load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(mistral_path, use_fast=False)\n",
        "\n",
        "# # Load model with quantization\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     mistral_path,\n",
        "#     quantization_config=bnb_config,  # Enables 8-bit quantization\n",
        "#     device_map=\"auto\",  # Automatically assigns model to available GPU(s)\n",
        "# )\n",
        "\n",
        "# print(\"Model and Tokenizer Loaded Successfully (8-bit Quantized)!\")\n"
      ],
      "metadata": {
        "id": "w5ZTjzGJU_Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# # Example input\n",
        "# input_text = \"What is the meaning of life?\"\n",
        "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# # Generate response\n",
        "# output = model.generate(input_ids, max_length=100)\n",
        "# response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# print(\"\\nGenerated Response:\")\n",
        "# print(response)\n"
      ],
      "metadata": {
        "id": "pMekBvZvV_rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mistral_inference transformers sentencepiece torch safetensors accelerate transformers trl torch peft wandb bitsandbytes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UppNgBKWUUxw",
        "outputId": "65ca0fcd-5323-4770-da25-ffbbf5bd49c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mistral_inference in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: fire>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (0.7.0)\n",
            "Requirement already satisfied: mistral_common>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (1.5.4)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (11.1.0)\n",
            "Requirement already satisfied: simple-parsing>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (0.1.7)\n",
            "Requirement already satisfied: xformers>=0.0.24 in /usr/local/lib/python3.11/dist-packages (from mistral_inference) (0.0.29.post3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.4.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.3)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.23.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.11.14)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.6.0->mistral_inference) (2.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: jsonschema>=4.21.1 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (4.23.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from mistral_common>=1.5.4->mistral_inference) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing>=0.1.5->mistral_inference) (0.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=3.0.0->trl) (1.18.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral_inference) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral_inference) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.21.1->mistral_common>=1.5.4->mistral_inference) (0.23.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
        "from trl import PPOTrainer, PPOConfig\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "q3jukxrXUO64",
        "outputId": "a1813ce6-d239-4c19-ddf1-3c8e112311f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'trl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-15f2c3873ad8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPPOConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "df = pd.read_json('/content/drive/MyDrive/PreProcessedDataset/train.json',lines = True)"
      ],
      "metadata": {
        "id": "Ok6nEXIWkPiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.head()\n",
        "dataset=df[\"source\"]\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "V454UA_ztvAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReinforcementModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # LLM model\n",
        "        self.mistral_path = \"/content/drive/MyDrive/mistral_models/7B-Instruct-v0.3\"\n",
        "\n",
        "        # Reward model\n",
        "        self.reward_model_name = \"OpenAssistant/reward-model\"\n",
        "\n",
        "        # 8-bit quantization config\n",
        "        bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "        # Pretrained tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.mistral_path, use_fast=False)\n",
        "\n",
        "        # Load pretrained model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.mistral_path,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "\n",
        "        # Device\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Load reward model\n",
        "        self.reward_model = self._load_reward_model()\n",
        "\n",
        "        # PPO config\n",
        "        ppo_config = PPOConfig(\n",
        "            model_name=self.mistral_path,\n",
        "            learning_rate=1.41e-5,\n",
        "            batch_size=4,\n",
        "            mini_batch_size=2,\n",
        "            gradient_accumulation_steps=8\n",
        "        )\n",
        "\n",
        "        # Initialize PPO Trainer\n",
        "        self.ppo_trainer = PPOTrainer(\n",
        "            model=self.model,\n",
        "            config=ppo_config,\n",
        "            dataset=df,\n",
        "            tokenizer=self.tokenizer,\n",
        "            reward_model=self.reward_model\n",
        "        )\n",
        "\n",
        "    def _load_reward_model(self):\n",
        "        \"\"\" Load the reward model \"\"\"\n",
        "        reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            self.reward_model_name,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        return reward_model\n",
        "\n",
        "    def generate_text(self, prompt):\n",
        "        \"\"\" Generate text using the model \"\"\"\n",
        "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self.device)\n",
        "        output = self.model.generate(input_ids, max_length=100)#write now its 100 (as it would require high computation) and GPU\n",
        "        response = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    def train(self, epochs=3):\n",
        "        \"\"\" Train the model using PPO \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            for dp in self.ppo_trainer.dataloader:\n",
        "                query = dp[\"source\"]  # Title/Prompt\n",
        "                response = self.generate_text(query)\n",
        "\n",
        "                # Compute reward (Using reward model)\n",
        "                inputs = self.tokenizer(response, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
        "                rewards = self.reward_model(**inputs).logits.squeeze()\n",
        "\n",
        "                # Train PPO step\n",
        "                self.ppo_trainer.step([query], [response], [rewards])\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs} completed!\")\n",
        "\n",
        "# Initialize Model\n",
        "rm = ReinforcementModel()\n",
        "\n",
        "# Train Model\n",
        "rm.train(epochs=3)\n"
      ],
      "metadata": {
        "id": "JEf4NauDSgMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "de7h-nJLtcfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}